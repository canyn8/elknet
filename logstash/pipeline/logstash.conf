input {
	syslog {
		port => 5140
		tags => [ "network" ]
	}
}


## Add your filters / logstash plugins configuration here
filter {
	#classify network syslog logs as cisco or other

		grok {
	        patterns_dir => "/opt/logstash/patterns"
	        #2014-06-26T18:05:06-07:00 10.8.60.66 62: Jun 26 18:05:05.129 PDT: %LINEPROTO-5-UPDOWN: Line protocol on Interface FastEthernet0/6, changed state to up
	        #The standard CISCOTIMESTAMP pattern does not patch the timezone so you will need to add the following pattern to a new pattern file in <logstash_home>/patterns/custom
	        #CISCOTIMESTAMPTZ %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME} %{TZ}
	        match => [ "message", "%{TIMESTAMP_ISO8601:syslog_ng_timestamp} %{IPORHOST:original_log_host} %{POSINT:cisco_number:int}: %{CISCOTIMESTAMPTZ:cisco_timestamp}: %%{DATA:facility}-%{POSINT:severity:int}-%{DATA:mnemonic}: %{GREEDYDATA:log_message}" ]
		}
    date {
	    timezone => "America/Los_Angeles"
	    #matches Cisco date/timestamps with timezone included. i.e - Jun 26 18:05:05.129 PDT
	    match => [ "cisco_timestamp", "MMM dd HH:mm:ss.SSS zzz" ]
		}
		if "_grokparsefailure" not in [tags] {
			#if grok parse was successful, then delete message field as we have already extracted the data into individual fields, and this would be redundant and ultimately
			#would take up twice the storage space.  But if grok parse failed, we want to keep the message field so we can correct our match expression.
			mutate {
				remove_field => [ "message" ]
			}
		}

} #end filter block


output {
	stdout { codec => rubydebug }
	file {
		path => "/var/log/network.log"
	}
	elasticsearch {
		hosts => "elasticsearch:9200"
		index => "syslog"
	}
}
